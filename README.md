# llm-engineer-upgrade
This repository is dedicated to providing resources, tools, and implementations for enhancing skills in Large Language Models (LLMs) and related fields. It includes projects and solutions that tackle various aspects of LLMs, such as fine-tuning, performance optimization, and deployment strategies.



## rope-handmake
RoPE-Handmade

This repository provides a from-scratch implementation of Rotary Position Embedding (RoPE), inspired by the paper RoFormer: Enhanced Transformer with Rotary Position Embedding
.
It is intended as a hands-on learning project to understand the transition from traditional positional encoding to RoPE.

### ðŸ“‚ Repository Structure
```bash
rope-handmake/
â”‚
â”œâ”€â”€ 01_ref/                # Reference materials
â”‚   â””â”€â”€ 2104.09864v5.pdf   # RoFormer paper
â”‚
â”œâ”€â”€ 02_origina-pe-core/    # Original positional encoding implementation
â”‚   â””â”€â”€ pe.py
â”‚
â”œâ”€â”€ 03_rope-core/          # Core RoPE implementation
â”‚   â””â”€â”€ rope.py
â”‚
â”œâ”€â”€ 04_debug/              # Debugging and test scripts
â”‚   â””â”€â”€ test-rope.py
```
### ðŸš€ Features

Step-by-step comparison between sinusoidal positional encoding and rotary position embedding.

Minimal implementation for clarity, without external dependencies.

Test scripts for validating the correctness of RoPE.

### ðŸ”§ Usage
-  Run the original positional encoding
python 02_origina-pe-core/pe.py

-  Run the RoPE implementation
python 03_rope-core/rope.py

-  Debug & test
python 04_debug/test-rope.py

### ðŸ“– Reference

RoFormer: Enhanced Transformer with Rotary Position Embedding (Su et al., 2021)



